import os
import cv2
import numpy as np
import tensorflow as tf
import base64
import eventlet
import time
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input
from tensorflow.keras.models import Model
from pythonosc.udp_client import SimpleUDPClient

import socket
import struct
import threading

# å…¨åŸŸ BPMï¼ˆçµ¦æ‰€æœ‰åœ°æ–¹ç”¨ï¼‰
current_bpm = 120.0

def bpm_listener():
    global current_bpm
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.bind(("0.0.0.0", 9001))   # è¦è·Ÿ Max ä¸€æ¨£çš„ port
    print("Listening raw BPM on UDP 9001 ...")
    while True:
        data, addr = sock.recvfrom(1024)
        try:
            # è©¦è©¦çœ‹ç´”æ–‡å­—æ ¼å¼
            try:
                text = data.decode("utf-8").strip()
                val = float(text)
                current_bpm = val
                print(f"[PY] BPM updated (text): {current_bpm}")
                continue
            except Exception:
                pass  # ä¸æ˜¯æ–‡å­—å°±å¾€ä¸‹

            # Max binary float æ ¼å¼ï¼šb'float....<4 bytes>'
            if data.startswith(b'float') and len(data) >= 4:
                bpm_bytes = data[-4:]
                val = struct.unpack('>f', bpm_bytes)[0]  # big-endian float
                current_bpm = float(val)
                print(f"[PY] BPM updated (binary): {current_bpm}")
            else:
                print("[PY] Unknown BPM packet:", data)

        except Exception as e:
            print("[PY] BPM parse error (unexpected):", data, e)

# åªè®“ BPM listener é–‹ä¸€æ¬¡
bpm_thread = None
def start_bpm_listener_once():
    global bpm_thread
    if bpm_thread is None or not bpm_thread.is_alive():
        bpm_thread = threading.Thread(target=bpm_listener, daemon=True)
        bpm_thread.start()
        print("[PY] BPM listener started.")
    else:
        print("[PY] BPM listener already running.")

# åˆå§‹åŒ– Flask å’Œ SocketIO
app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!'
socketio = SocketIO(app, cors_allowed_origins="*")

# è¨­å®šè·¯å¾‘
UPLOAD_FOLDER = 'uploads'
MODEL_PATH = 'emotion_detection_model_100epochs.h5'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# OSC è¨­å®šï¼ˆç”¨æ–¼ Max/MSP é€šè¨Šï¼‰
OSC_IP = "127.0.0.1"
OSC_PORT = 8000
SEND_INTERVAL = 0.5  # æ¯ 0.5 ç§’ç™¼é€ä¸€æ¬¡ OSC è¨Šæ¯

# å…¨åŸŸè®Šæ•¸å„²å­˜æ¨¡å‹
model = None
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']
emotion_colors = {
    'Angry': (0, 0, 255),
    'Disgust': (0, 255, 0),
    'Fear': (128, 0, 128),
    'Happy': (0, 255, 255),
    'Neutral': (128, 128, 128),
    'Sad': (255, 0, 0),
    'Surprise': (180, 105, 255)
}
osc = SimpleUDPClient("127.0.0.1", 8000)

# åˆå§‹åŒ– OSC å®¢æˆ¶ç«¯
osc_client = None
try:
    osc_client = SimpleUDPClient(OSC_IP, OSC_PORT)
    print(f"OSC client initialized: {OSC_IP}:{OSC_PORT}")
except Exception as e:
    print(f"Warning: Could not initialize OSC client: {e}")
    print("OSC messages will not be sent to Max/MSP")

# --- é€™æ˜¯ä½ åŸæœ¬ ipynb ä¸­çš„æ¨¡å‹å»ºæ§‹é‚è¼¯ (ç‚ºäº†ç¢ºä¿æ¨¡å‹è¼‰å…¥æ­£ç¢º) ---
def create_model_architecture(input_shape=(48, 48, 1), num_classes=7):
    inputs = Input(shape=input_shape)
    x = Conv2D(32, kernel_size=(3, 3), activation='relu', name="conv_1")(inputs)
    x = Conv2D(64, kernel_size=(3, 3), activation='relu', name="conv_2")(x)
    x = MaxPooling2D(pool_size=(2, 2), name="pool_1")(x)
    x = Dropout(0.1, name="dropout_1")(x)
    x = Conv2D(128, kernel_size=(3, 3), activation='relu', name="conv_3")(x)
    x = MaxPooling2D(pool_size=(2, 2), name="pool_2")(x)
    x = Dropout(0.1, name="dropout_2")(x)
    x = Conv2D(256, kernel_size=(3, 3), activation='relu', name="conv_4")(x)
    x = MaxPooling2D(pool_size=(2, 2), name="pool_3")(x)
    x = Dropout(0.1, name="dropout_3")(x)
    x = Flatten(name="flatten")(x)
    x = Dense(512, activation='relu', name="dense_1")(x)
    x = Dropout(0.2, name="dropout_4")(x)
    outputs = Dense(num_classes, activation='softmax', name="output")(x)
    return Model(inputs=inputs, outputs=outputs, name="emotion_model")

def load_emotion_model():
    global model
    if os.path.exists(MODEL_PATH):
        try:
            # å˜—è©¦ç›´æ¥è¼‰å…¥
            model = tf.keras.models.load_model(MODEL_PATH)
            print("Model loaded successfully via load_model.")
        except Exception as e:
            print(f"Direct load failed, trying to build architecture first: {e}")
            # å¦‚æœç›´æ¥è¼‰å…¥å¤±æ•—ï¼Œå…ˆå»ºæ§‹æ¶æ§‹å†è¼‰å…¥æ¬Šé‡ (æ ¹æ“šä½ çš„ ipynb é‚è¼¯)
            model = create_model_architecture()
            model.load_weights(MODEL_PATH)
            print("Model weights loaded successfully.")
    else:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ {MODEL_PATH}ã€‚è«‹ç¢ºä¿æª”æ¡ˆå­˜åœ¨ã€‚")

# é æ¸¬å‡½æ•¸
def predict_emotion(face_img):
    resized_face = cv2.resize(face_img, (48, 48)) / 255.0
    face_input = np.expand_dims(resized_face, axis=(0, -1))
    predictions = model.predict(face_input, verbose=0)
    return predictions[0]

# --- Flask Routes ---

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
    
    filename = 'uploaded_video.mp4'
    filepath = os.path.join(UPLOAD_FOLDER, filename)
    file.save(filepath)
    return jsonify({'message': 'File uploaded successfully', 'filepath': filepath})

# --- SocketIO Events ---

@socketio.on('start_processing')
def handle_process_video(data):
    global model, osc_client

    # å¾å‰ç«¯æ‹¿æ¨¡å¼ï¼ˆupload / cameraï¼‰
    mode = data.get('mode', 'upload')
    print('[PY] start_processing, mode =', mode)

    if model is None:
        load_emotion_model()
        if model is None:
            emit('error', {'msg': 'Model not found!'})
            return

    # ğŸš¨ ç›®å‰åªå¯¦ä½œäº†ä¸Šå‚³å½±ç‰‡çš„æµç¨‹
    if mode != 'upload':
        emit('error', {'msg': f'Mode {mode} not implemented yet. è«‹å…ˆç”¨ä¸Šå‚³å½±ç‰‡æ¨¡å¼ã€‚'})
        return

    video_path = os.path.join(UPLOAD_FOLDER, 'uploaded_video.mp4')
    if not os.path.exists(video_path):
        emit('error', {'msg': 'Video file not found. Please upload first.'})
        return

    # ---------- Phase1ï¼šå…ˆè·‘å®Œæ•´éƒ¨å½±ç‰‡ï¼Œæ”¶é›† all_probs + face_boxes ----------
    cap = cv2.VideoCapture(video_path)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +
                                         'haarcascade_frontalface_default.xml')

    print("Starting video processing (Phase1: precompute probs)...")

    if not cap.isOpened():
        emit('error', {'msg': 'Cannot open video file.'})
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print("FPS:", fps, "Total frames:", total_frames)

    all_probs = []        # æ¯å¹€çš„æƒ…ç·’æ©Ÿç‡ (N, 7)
    face_boxes = []       # æ¯å¹€çš„è‡‰æ¡† (x, y, w, h) æˆ– None

    last_prob = np.ones(len(emotion_labels)) / len(emotion_labels)
    last_box = None

    SAMPLE_STEP = 5
    frame_idx = 0

    def predict_emotion_with_probabilities(face_bgr, model):
        face_gray = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2GRAY)
        resized_face = cv2.resize(face_gray, (48, 48)) / 255.0
        face_img = np.expand_dims(resized_face, axis=(0, -1))
        predictions = model.predict(face_img, verbose=0)
        return predictions[0]

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % SAMPLE_STEP == 0:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

            if len(faces) > 0:
                x, y, w, h = faces[0]
                face = frame[y:y + h, x:x + w]
                prob = predict_emotion_with_probabilities(face, model)
                last_prob = prob
                last_box = (int(x), int(y), int(w), int(h))
            else:
                prob = last_prob
                last_box = None
        else:
            prob = last_prob

        all_probs.append(prob)
        face_boxes.append(last_box)

        if frame_idx % 50 == 0:
            print(f"Phase1: Processed frame {frame_idx}/{total_frames}")
        frame_idx += 1

    cap.release()

    all_probs = np.stack(all_probs)  # shape = (N, 7)
    N = all_probs.shape[0]
    print("Collected probs for frames:", N)

    # ---------- Phase2ï¼šé‡æ–°æ’­æ”¾å½±ç‰‡ï¼Œç”¨ all_probs åš NOW + æœªä¾†å››æ‹ ----------
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        emit('error', {'msg': 'Cannot reopen video file for Phase2.'})
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    N = all_probs.shape[0]
    frame_idx = 0
    last_osc_time = -1e9

    print(f"[PY] Video FPS = {fps}, frames in probs = {N}")
    print("Starting Phase2: playback with 4-beat forecast...")

    try:
        while True:
            ret, frame = cap.read()
            if not ret or frame_idx >= N:
                print("[PY] Video ended.")
                break

            current_time_sec = frame_idx / fps

            # ===== NOWï¼šé€™å¹€çš„ 7 ç¶­æƒ…ç·’ =====
            probs_now = all_probs[frame_idx]
            idx_now = int(np.argmax(probs_now))
            emotion_now = emotion_labels[idx_now]
            prob_now = float(probs_now[idx_now])

            # ===== FUTUREï¼šæœªä¾†å››æ‹å¹³å‡ =====
            safe_bpm = max(current_bpm, 1e-3)
            beat_seconds = 60.0 / safe_bpm
            four_beat_seconds = beat_seconds * 4.0

            window_frames = int(fps * four_beat_seconds)
            if window_frames < 1:
                window_frames = 1
            start = frame_idx + 1
            end = min(N, frame_idx + 1 + window_frames)

            if start < end:
                window = all_probs[start:end]
                probs_future = window.mean(axis=0)
            else:
                probs_future = probs_now.copy()

            idx_future = int(np.argmax(probs_future))
            emotion_future = emotion_labels[idx_future]
            prob_future = float(probs_future[idx_future])

            # ===== æ¯ä¸€æ‹é€ä¸€æ¬¡ OSC =====
            if current_time_sec - last_osc_time >= beat_seconds:
                if osc_client is not None:
                    osc_client.send_message("/emotion_label_future",[emotion_future, prob_future, float(four_beat_seconds)])

                    for label, p in zip(emotion_labels, probs_future):
                        osc_client.send_message("/emotion_prob_future", [label, float(p)])

                    osc_client.send_message("/emotion_label", [emotion_now, prob_now])

                    for label, p in zip(emotion_labels, probs_now):
                        osc_client.send_message("/emotion_prob", [label, float(p)])

                last_osc_time = current_time_sec

            # ===== ç•«è‡‰æ¡† =====
            box = face_boxes[frame_idx] if frame_idx < len(face_boxes) else None
            if box is not None:
                x, y, w, h = box
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)

            # ===== ç•«æ–‡å­— =====
            cv2.putText(frame, f"NOW: {emotion_now} {prob_now*100:.1f}%",
                        (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
            cv2.putText(frame,
                        f"FUT[4 beats ~ {four_beat_seconds:.2f}s]: {emotion_future} {prob_future*100:.1f}%",
                        (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,255), 2)

            h, w, _ = frame.shape
            line_height = 20
            x_left = 20
            for i, (label, p) in enumerate(zip(emotion_labels, probs_now)):
                text = f"{label}: {p*100:.1f}%"
                y = h - 20 - i * line_height
                cv2.putText(frame, text, (x_left, y),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)

            # ===== ä¸Ÿçµ¦å‰ç«¯ =====
            frame = cv2.resize(frame, None, fx=0.6, fy=0.6)
            _, buffer = cv2.imencode('.jpg', frame)
            frame_base64 = base64.b64encode(buffer).decode('utf-8')
            socketio.emit('video_frame', {'image': frame_base64})

            emotion_data = {label: float(p)*100 for label, p in zip(emotion_labels, probs_now)}
            socketio.emit('emotion_update', emotion_data)

            socketio.sleep(1.0 / fps)
            frame_idx += 1

    finally:
        cap.release()

    emit('processing_complete', {'msg': 'å½±ç‰‡åˆ†æå®Œæˆ', 'mode': mode})
    print("Video processing finished and released.")

if __name__ == '__main__':
    # ç¬¬ä¸€æ¬¡å•Ÿå‹•æ™‚å˜—è©¦è¼‰å…¥æ¨¡å‹
    load_emotion_model()
    print("Server starting on http://127.0.0.1:5000")
    socketio.run(app, debug=True, port=5000)